{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.3\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "from ck_image_helpers import *\n",
    "from hanziconv import HanziConv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import itertools\n",
    "from perimetric_complexity import perimetricComplexity\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# preprocessing parameters\n",
    "STRETCH = False\n",
    "SKELETONISE_METHOD = \"lee\"\n",
    "IMG_SIZE = 300\n",
    "PERIODS = [\"Oracle\", \"Bronze\", \"Seal\", \"Simplified\", \"Traditional\"]\n",
    "\n",
    "# folder locations\n",
    "ROOT = \"/Users/ckemp/bigdatanonarchival/hanzi/smallprocessingtest_preprint\"\n",
    "#ROOT = \"/Users/ckemp/bigdatanonarchival/hanzi/processingtest\"\n",
    "IMAGES = f\"{ROOT}/Images\"\n",
    "DATA = f\"{ROOT}/Data\"\n",
    "IMAGE_DATASETS = [\"hanziyuan\", \"simplified_handwritten\", \"traditional_handwritten\"] #, \"hanziyuan, simplified_handwritten\", \"traditional_handwritten\"]\n",
    "#IMAGE_DATASETS = [\"hanziyuan\"]\n",
    "CLD = f\"{DATA}/cld.csv\"\n",
    "PATHOFF=3\n",
    "\n",
    "# # to generate experimental stimuli\n",
    "# SKELETONISE_METHOD = \"none\"\n",
    "# IMAGES = \"/Users/ckemp/u/mygithub/hanzi/experiments/exp_pictographic_ratings/stimuli/stimuliprocessing\"\n",
    "# IMAGE_DATASETS = [\"oracle\", \"traditional\", \"hwtrad\", \"hwtrad_control\", \"traditional_control\"]\n",
    "# PATHOFF=1\n",
    "\n",
    "# # to generate images for demo figure\n",
    "# SKELETONISE_METHOD = \"none\"\n",
    "# IMAGES = \"/Users/ckemp/u/mygithub/hanzi/projectdocs/figures/demofig/Images\"\n",
    "# IMAGE_DATASETS = [\"hanziyuan\", \"traditional_handwritten\", \"simplified_handwritten\"]\n",
    "# PATHOFF=1\n",
    "\n",
    "# # to generate images for nn figure\n",
    "# SKELETONISE_METHOD = \"none\"\n",
    "# IMAGES = \"/Users/ckemp/u/mygithub/hanzi/projectdocs/figures/nnfig/Images\"\n",
    "# IMAGE_DATASETS = [\"hanziyuan\", \"traditional_handwritten\", \"simplified_handwritten\"]\n",
    "# PATHOFF=1\n",
    "import skimage\n",
    "print(skimage.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor stuff we need\n",
    "def check_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "        \n",
    "def listdir(x):\n",
    "    return [i for i in os.listdir(x) if i[0] != \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy images\n",
    "\n",
    "def copy_image(from_image_path, to_folder):\n",
    "\n",
    "    to_image_path = f\"{to_folder}/{'_'.join(from_image_path.split('.')[0].split('/')[-PATHOFF:])}.png\"\n",
    "\n",
    "    if from_image_path.split(\".\")[1] == \"svg\":\n",
    "        svg_to_png(from_image_path, to_image_path)\n",
    "    else:\n",
    "        copyfile(from_image_path, to_image_path)\n",
    "\n",
    "for dataset in IMAGE_DATASETS:\n",
    "    raw_folder = f\"{IMAGES}/{dataset}_raw\"\n",
    "    processed_folder = f\"{IMAGES}/{dataset}_{'stretched' if STRETCH else 'padded'}_{SKELETONISE_METHOD}\"\n",
    "\n",
    "    check_create_folder(processed_folder)\n",
    "\n",
    "    image_paths = get_all_image_paths(raw_folder)\n",
    "    image_paths = [i.replace(\"\\\\\", \"/\") for i in image_paths]\n",
    "    \n",
    "    list(itertools.starmap(copy_image, [(image_path, processed_folder) for image_path in image_paths]))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean images: ie delete images that have no black pixels\n",
    "\n",
    "\n",
    "for dataset in IMAGE_DATASETS:\n",
    "    processed_folder = f\"{IMAGES}/{dataset}_{'stretched' if STRETCH else 'padded'}_{SKELETONISE_METHOD}\"\n",
    "    image_paths = get_all_image_paths(processed_folder)\n",
    "\n",
    "    list(map(clean_image, image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: /Users/ckemp/bigdatanonarchival/hanzi/smallprocessingtest_preprint/Images/hanziyuan_padded_lee/丁_Bronze_B20296.png is a low contrast image\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: /Users/ckemp/bigdatanonarchival/hanzi/smallprocessingtest_preprint/Images/hanziyuan_padded_lee/丁_Bronze_B20293.png is a low contrast image\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: /Users/ckemp/bigdatanonarchival/hanzi/smallprocessingtest_preprint/Images/hanziyuan_padded_lee/丁_Bronze_B20292.png is a low contrast image\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: /Users/ckemp/bigdatanonarchival/hanzi/smallprocessingtest_preprint/Images/simplified_handwritten_padded_lee/simplified_handwritten_raw_绣_83.png is a low contrast image\n"
     ]
    }
   ],
   "source": [
    "# process images\n",
    "\n",
    "def process_image(image_path, stretch=STRETCH, skeletonise_method=SKELETONISE_METHOD, size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Universal image processing function:\n",
    "    1. Binarise image\n",
    "    2. Crop image to become square, scale to standardised dimensions\n",
    "    3. Skeletonise image\n",
    "    \"\"\"\n",
    "\n",
    "    image = img_as_ubyte(io.imread(image_path, as_gray=True))\n",
    "\n",
    "    # Binarise image\n",
    "    image = np.array([np.array([True if px != 255 else False for px in row]) for row in image])\n",
    "\n",
    "    # Crop and scale image (stretching not allowed yet)\n",
    "    image = crop_and_scale_image(image, False)\n",
    "\n",
    "    # Scale image to standard size and skeletonise\n",
    "    image = rescale_and_skeletonise_image_variant(image, size, stretch, skeletonise_method)\n",
    "\n",
    "    io.imsave(image_path, img_as_ubyte(image))\n",
    "    \n",
    "\n",
    "for dataset in IMAGE_DATASETS:\n",
    "\n",
    "    processed_folder = f\"{IMAGES}/{dataset}_{'stretched' if STRETCH else 'padded'}_{SKELETONISE_METHOD}\"\n",
    "\n",
    "    image_paths = get_all_image_paths(processed_folder)\n",
    "    image_paths = [i.replace(\"\\\\\", \"/\") for i in image_paths]\n",
    "\n",
    "    list(map(process_image, image_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data into CSV\n",
    "\n",
    "def get_complexity_and_meta(image_path):\n",
    "    ip_split = image_path.split(\".\")[0].split(\"/\")[-1].split(\"_\")\n",
    "    dataset = \"_\".join(image_path.split(\"/\")[-2].split(\"_\")[:-2])\n",
    "    perimetric, pixel = perimetricComplexity(image_path)\n",
    "    return ip_split, dataset, perimetric, pixel\n",
    "\n",
    "def get_hanzi_image_complexity(image_path):\n",
    "    ip_split, dataset, perimetric, pixel = get_complexity_and_meta(image_path)\n",
    "    return ip_split[-3], ip_split[-2], ip_split[-1], 'stretch' if STRETCH else 'pad', SKELETONISE_METHOD, dataset, perimetric, pixel\n",
    "\n",
    "def get_simph_image_complexity(image_path):\n",
    "    ip_split, dataset, perimetric, pixel = get_complexity_and_meta(image_path)\n",
    "    return ip_split[-2], \"Simplified\", ip_split[-1], 'stretch' if STRETCH else 'pad', SKELETONISE_METHOD, dataset, perimetric, pixel\n",
    "\n",
    "def get_tradh_image_complexity(image_path):\n",
    "    ip_split, dataset, perimetric, pixel = get_complexity_and_meta(image_path)\n",
    "    return ip_split[-2], \"Traditional\", ip_split[-1], 'stretch' if STRETCH else 'pad', SKELETONISE_METHOD, dataset, perimetric, pixel\n",
    "\n",
    "cld = pd.read_csv(CLD, index_col=0)\n",
    "cld_characters = set(cld[\"Word\"])\n",
    "\n",
    "all_rows = []\n",
    "columns = [\"character\", \"period\", \"ID\", \"scale method\", \"skeletonise method\", \"dataset\", \"perimetric complexity\", \"pixel complexity\"]\n",
    "\n",
    "for dataset in IMAGE_DATASETS:\n",
    "\n",
    "    folder = f\"{IMAGES}/{dataset}_{'stretched' if STRETCH else 'padded'}_{SKELETONISE_METHOD}\"\n",
    "    images = listdir(folder)\n",
    "\n",
    "\n",
    "    if dataset == \"hanziyuan\":\n",
    "        func = get_hanzi_image_complexity\n",
    "        cld_images = [f\"{folder}/{image}\" for image in images if image.split(\"_\")[0] in cld_characters and image.split(\"_\")[1] in PERIODS]\n",
    "    elif dataset == \"simplified_handwritten\":\n",
    "        func = get_simph_image_complexity\n",
    "        cld_images = [f\"{folder}/{image}\" for image in images if image.split(\"_\")[-2] in cld_characters]\n",
    "    else:\n",
    "        # dataset == \"traditional_handwritten\"\n",
    "        func = get_tradh_image_complexity\n",
    "        cld_images = [f\"{folder}/{image}\" for image in images if HanziConv.toSimplified(image.split(\"_\")[-2]) in cld_characters]\n",
    "\n",
    "    rows = list(map(func, cld_images))\n",
    "\n",
    "    all_rows += rows\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    csv_path = f\"{DATA}/{dataset}_complexities.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        df1 = pd.read_csv(csv_path, index_col=0)\n",
    "        df_out = pd.concat([df1, df], ignore_index=True)\n",
    "        df_out.drop_duplicates().reset_index().to_csv(csv_path)\n",
    "    else:\n",
    "        df.to_csv(csv_path)\n",
    "\n",
    "df = pd.DataFrame(all_rows, columns=columns)\n",
    "csv_path = f\"{DATA}/all_complexities.csv\"\n",
    "if os.path.exists(csv_path):\n",
    "    df1 = pd.read_csv(csv_path, index_col=0)\n",
    "    df_out = pd.concat([df1, df], ignore_index=True)\n",
    "    df_out.drop_duplicates().reset_index().to_csv(csv_path)\n",
    "else:\n",
    "    df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
